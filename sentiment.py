# -*- coding: utf-8 -*-
"""sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UjowVSoc0g2A49YqZDPKiLoIi-OGdGkq
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import utils
import importlib
importlib.reload(utils)
from utils import *

import re, string, collections, bcolz, pickle, os
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import TruncatedSVD, PCA
from sklearn.metrics import confusion_matrix

"""The objective of this notebook is to train a sentiment analysis classifier given the games reviews in the dataset. The model must classify text reviews in positive or negative"""

df = pd.read_csv('data/reviews.csv', lineterminator='\n')

df.shape

"""There's 127699 games reviews"""

df.head()

"""Let's keep just text and score columns"""

df = df[['text', 'score']]

df.head()

"""The data have 127699 reviews but we will keep just the most polarized ones. The filter is the next: <br>Positive: score > 85 <br/>Negative: score < 50"""

print(f'reviews with score greater than 85: {len(df.loc[df["score"] > 85])}')
print(f'reviews with score less than 50: {len(df.loc[df["score"] < 50])}')

pos = df.loc[df['score'] > 85, 'text'].copy().reset_index(drop=True)
neg = df.loc[df['score'] < 50, 'text'].copy().reset_index(drop=True)

len(pos), len(neg)

"""Let's print some positive reviews examples"""

for i in range(4):
    print(''.join(pos[np.random.randint(0, len(pos))]))
    print('\n')

"""Let's print some negative reviews examples"""

for i in range(4):
    print(''.join(neg[np.random.randint(0, len(neg))]))
    print('\n')

"""We must add the labels: 0 for negative reviews, 1 for positive reviews"""

neg = pd.concat([pd.DataFrame(neg), pd.DataFrame(np.zeros(neg.shape), columns=['class'])], 1)
pos = pd.concat([pd.DataFrame(pos), pd.DataFrame(np.ones(pos.shape), columns=['class'])], 1)

"""Mean, standard deviation and max length of negative reviews"""

lens = neg['text'].str.len()
lens.mean(), lens.std(), lens.max()

lens.hist(figsize=(12, 6), bins=25);

"""Reviews with more than 5000 characters are dropped"""

long_reviews = neg.loc[neg['text'].str.len() > 5000].index
neg.drop(long_reviews, inplace=True)

"""Mean, standard deviation and max length of positive reviews"""

lens = pos['text'].str.len()
lens.mean(), lens.std(), lens.max()

lens.hist(figsize=(12, 6), bins=25);

long_reviews = pos.loc[pos['text'].str.len() > 5000].index
pos.drop(long_reviews, inplace=True)

"""Is desirable to have a balanced dataset (similar quantity of positive and negative instances). So we will pick a random subset of the positive instances."""

np.random.seed(42)
rand = np.random.permutation(pos.shape[0])
pos = pos.iloc[rand[:neg.shape[0]]].reset_index(drop=True)

neg.shape, pos.shape

"""We concatenate positive and negative reviews"""

df = pd.concat([pos, neg]).sample(frac=1).reset_index(drop=True)
df.head()

df.shape

"""Split data into train and test set"""

X_train, X_test, y_train, y_test = train_test_split(df['text'].values, df['class'].values, test_size=0.2, random_state=42)

len(X_train), len(X_test), len(y_train), len(y_test)

"""Defining tokenizer"""

re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')
def tokenize(s): return re_tok.sub(r' \1 ', s).split()

"""Creating bag of words"""

vect = CountVectorizer(tokenizer=tokenize)

tf_train = vect.fit_transform(X_train)
tf_test = vect.transform(X_test)

"""Train term frequency contains 32675 documents and 45424 tokens. Each row represents a document and each column how many times that token appears on the document."""

tf_train

"""For instance, the first document have just 65 of the 45424 possible tokens. That's why the matrix is stored as a sparse matrix."""

tf_train[0]

"""Get vocabulary"""

vocab = vect.get_feature_names()
len(vocab)

vocab[4000: 4005]

X_train[0]

"""First review's tokens"""

w0 = set([o for o in X_train[0].split(' ')])

w0

vect.vocabulary_['unless']

tf_train[0, 41989]

"""### Multinomial Naive Bayes

Here we reduce our training data to 2 dimentions (using TruncatedSVD rather than PCA because we are working with sparse matrices) and then we plot it in order to  roughly see how classes are separated
"""

svd = TruncatedSVD()
reduced_tf_train = svd.fit_transform(tf_train)

plot_embeddings(reduced_tf_train, y_train)

"""First attempt is to use a simple Multinomial Naive Bayes"""

p = tf_train[y_train==1].sum(0) + 1
q = tf_train[y_train==0].sum(0) + 1
r = np.log((p/p.sum())/(q/q.sum()))
b = np.log(len(p)/len(q))

pre_preds = tf_test @ r.T + b
preds = pre_preds.T > 0
acc = (preds==y_test).mean()
print(f'Accuracy: {acc}')

"""Rather than use r coefficients that we get trough Naive Bayes, let's learn them from data using Logistic Regression"""

model = LogisticRegression(C=0.2, dual=True)
model.fit(tf_train, y_train)
preds = model.predict(tf_test)
acc = (preds==y_test).mean()
print(f'Accuracy: {acc}')

plot_confusion_matrix(confusion_matrix(y_test, preds.T), classes=['Negative', 'Positive'], title='Confusion matrix')

"""Let's plot most relevant words that the algorithm uses to classify a text in positive or negative"""

coef_df = pd.DataFrame({'vocab': vocab, 'coef':model.coef_.reshape(-1)})
pos_top10 = coef_df.sort_values('coef', ascending=False).reset_index(drop=True)[:10]
neg_top10 = coef_df.sort_values('coef').reset_index(drop=True)[:10]

fig, axs = plt.subplots(1, 2, figsize=(8, 8))
fig.subplots_adjust(wspace=0.8)
pos_top10.sort_values('coef').plot.barh(legend=False, ax=axs[0])
axs[0].set_yticklabels(pos_top10['vocab'].values.tolist()[::-1])
axs[0].set_title('Positive');
neg_top10.sort_values('coef', ascending=False).plot.barh(legend=False, ax=axs[1])
axs[1].set_yticklabels(neg_top10['vocab'].values.tolist()[::-1])
axs[1].set_title('Negative');

"""Removing 'ea' token"""

vect.vocabulary_['ea']

tf_train = lil2(tf_train)
tf_train.removecol(12893)

tf_test = lil2(tf_test)
tf_test.removecol(12893)

model = LogisticRegression(C=0.2, dual=True)
model.fit(tf_train, y_train)
preds = model.predict(tf_test)
acc = (preds==y_test).mean()
print(f'Accuracy: {acc}')

"""Let's try now with bigram tf-idf rather than unigram tf"""

vect = TfidfVectorizer(strip_accents='unicode', tokenizer=tokenize, ngram_range=(1, 2), max_df=0.9, min_df=3, sublinear_tf=True)

tfidf_train = vect.fit_transform(X_train)
tfidf_test = vect.transform(X_test)

"""Division between classes is more clear now"""

svd = TruncatedSVD()
reduced_tfidf_train = svd.fit_transform(tfidf_train)

plot_embeddings(reduced_tfidf_train, y_train, 2000)

p = tfidf_train[y_train==1].sum(0) + 1
q = tfidf_train[y_train==0].sum(0) + 1
r = np.log((p/p.sum())/(q/q.sum()))
b = np.log(len(p)/len(q))

model = LogisticRegression(C=30, dual=True)
model.fit(tfidf_train, y_train)
preds = model.predict(tfidf_test)
acc = (preds==y_test).mean()
print(f'Accuracy: {acc}')

plot_confusion_matrix(confusion_matrix(y_test, preds.T), classes=['Negative', 'Positive'], title='Confusion matrix')

"""### CNN

We will try now with a simple Convolutional Neural Network
"""

from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential, Model
from keras.layers.embeddings import Embedding
from keras.layers import Flatten, Dense, Dropout, Convolution1D, MaxPooling1D, SpatialDropout1D, Input, concatenate
from keras.optimizers import Adam

"""Tokenize reviews"""

df['tokenized'] = df['text'].apply(tokenize)

"""Get vocabulary"""

def update_vocab_counter(row):
    for word in row:
        vocab_counter[word] += 1

vocab_counter = collections.Counter()
df['tokenized'].apply(update_vocab_counter);
vocab = sorted(vocab_counter, key=vocab_counter.get, reverse=True)

len(vocab)

"""Word to index dictionary"""

w2id = {w:i for i, w in enumerate(vocab)}

"""Words to index transformation"""

def transform_to_ids(row):
    return [w2id[w] for w in row]

df['tokenized'] = df['tokenized'].apply(lambda x: transform_to_ids(x))

X_train, X_test, y_train, y_test = train_test_split(df['tokenized'].values, df['class'].values, test_size=0.2, random_state=42)

maxlen = 1000

"""All the inputs to the conv net must have a fixed size. We set 1000 as the max length and we fill with -1 all reviews smaller than 1000."""

x_train = pad_sequences(X_train, maxlen=maxlen, value=-1)
x_test = pad_sequences(X_test, maxlen=maxlen, value=-1)

model = Sequential([Embedding(input_dim=len(vocab), output_dim=32, input_length=maxlen),
                    SpatialDropout1D(0.2),
                    Dropout(0.25),
                    Convolution1D(64, 5, padding='same', activation='relu'),
                    Dropout(0.25),
                    MaxPooling1D(),
                    Flatten(),
                    Dense(100, activation='relu'),
                    Dropout(0.85),
                    Dense(1, activation='sigmoid')])

model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])

model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=4, batch_size=64)

"""Maybe finding best hyperparameters could lead to a better accuracy, but this more complex model is not better than our linear classifier."""

